{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "G3MLALzGPBMI",
    "outputId": "d5c547c3-6900-42bb-db9b-98fb7a47af71"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-19529e016bd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'helpers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'helpers.data'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from random import randint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "if not os.path.exists('helpers'):\n",
    "    !git clone https://github.com/solanyn/head-pose-estimation\n",
    "    !mv head-pose-estimation/helpers .\n",
    "    !rm -r head-pose-estimation\n",
    "sys.path.append('helpers')\n",
    "\n",
    "from helpers.data import *\n",
    "from helpers.model import *\n",
    "from helpers.plot import *\n",
    "\n",
    "print(\"Tensorflow version is:\", tf.__version__)\n",
    "assert tf.__version__[0] == '2'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "models = []\n",
    "\n",
    "colab = True\n",
    "if colab:\n",
    "    print(\"Running on Google Colab (Importing data from Google Drive)\")\n",
    "else:\n",
    "    print(\"Running locally (Data available locally)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZd_UzSePBMN"
   },
   "outputs": [],
   "source": [
    "logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
    "shutil.rmtree(logdir, ignore_errors=True)\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Open an embedded TensorBoard viewer\n",
    "%tensorboard --logdir {logdir}/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zg0P-0ORPFEi"
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    !cp /content/drive/'My Drive'/colab/head-pose-estimation/data.zip .\n",
    "    !unzip -o -q data.zip\n",
    "    !rm data.zip\n",
    "    !unzip -o -q modified_data.zip\n",
    "    !rm modified_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBjMR1UTPBMT"
   },
   "outputs": [],
   "source": [
    "data_dir = './modified_data/'\n",
    "\n",
    "train_data = pd.read_csv('./train_data.csv')\n",
    "test_data = pd.read_csv('./test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "805KWXiGPBMV"
   },
   "outputs": [],
   "source": [
    "test_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SkxUSTqDPBMX"
   },
   "outputs": [],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I55hW77gPBMa"
   },
   "outputs": [],
   "source": [
    "# Look at a random image in the training set and the associated labels\n",
    "index = randint(0, len(train_data))\n",
    "fig = plt.figure()\n",
    "im = plt.imread(data_dir + train_data['filename'][index])\n",
    "plt.imshow(im)\n",
    "\n",
    "print(\"Tilt: {}\\nPan: {}\\nSize: {}\\nTotal images: {}\".format(train_data[\"tilt\"][index], train_data[\"pan\"][index], im.shape, len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d43yjEAaPBMf"
   },
   "outputs": [],
   "source": [
    "# Are our labels categorical or numerical?\n",
    "train_data['pan_str'] = train_data['pan'].astype(str)\n",
    "train_data['tilt_str'] = train_data['tilt'].astype(str)\n",
    "\n",
    "train_data['pan_str'].value_counts().plot(kind='bar')\n",
    "plt.title('Tilt Class Counts')\n",
    "plt.show()\n",
    "plt.title('Pan Class Counts')\n",
    "train_data['tilt_str'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsOz1TyDPBMi"
   },
   "outputs": [],
   "source": [
    "# Definitely categorical. Some imbalances in both classes more apparent in the pan classes.\n",
    "\n",
    "num_tilt_classes = len(train_data['tilt'].unique())\n",
    "num_pan_classes = len(train_data['pan'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "by8Ta4gFPBMk"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_data_df, val_data_df = train_test_split(train_data, test_size=0.2)\n",
    "\n",
    "TRAIN_DATA_LEN = len(train_data_df)\n",
    "VAL_DATA_LEN = len(val_data_df)\n",
    "TEST_DATA_LEN = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVSSKYVwPBMm"
   },
   "outputs": [],
   "source": [
    "# Tensorflow Datasets\n",
    "tilt_train_dataset = tf.data.Dataset.from_generator(\n",
    "    make_tilt_train_generator\n",
    "    , output_types=(tf.float32,tf.float32)\n",
    "    , output_shapes = ((1,64,64,3), (1, num_tilt_classes))\n",
    ")\n",
    "\n",
    "pan_train_dataset = tf.data.Dataset.from_generator(\n",
    "    make_pan_train_generator\n",
    "    , output_types=(tf.float32,tf.float32)\n",
    "    , output_shapes = ((1,64,64,3), (1, num_pan_classes))\n",
    ")\n",
    "\n",
    "tilt_val_dataset = tf.data.Dataset.from_generator(\n",
    "    make_tilt_val_generator\n",
    "    , output_types=(tf.float32,tf.float32)\n",
    "    , output_shapes = ((1,64,64,3), (1, num_tilt_classes))\n",
    ")\n",
    "\n",
    "pan_val_dataset = tf.data.Dataset.from_generator(\n",
    "    make_pan_val_generator\n",
    "    , output_types=(tf.float32,tf.float32)\n",
    "    , output_shapes = ((1,64,64,3), (1, num_pan_classes))\n",
    ")\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    make_test_generator\n",
    "    , output_types=(tf.float32,tf.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7HyGSvGPBMp"
   },
   "outputs": [],
   "source": [
    "for image, label in tilt_train_dataset.take(5):\n",
    "    print(image.shape, label.shape)\n",
    "\n",
    "for image, label in pan_train_dataset.take(5):\n",
    "    print(image.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ci6clTHhPBMu"
   },
   "outputs": [],
   "source": [
    "# No aug\n",
    "tilt_train_batches = (\n",
    "    tilt_train_dataset\n",
    "    .take(TRAIN_DATA_LEN)\n",
    "    .cache()\n",
    "    .shuffle(TRAIN_DATA_LEN)\n",
    "    .map(convert, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ") \n",
    "\n",
    "pan_train_batches = (\n",
    "    pan_train_dataset\n",
    "    .take(TRAIN_DATA_LEN)\n",
    "    .cache()\n",
    "    .shuffle(TRAIN_DATA_LEN)\n",
    "    .map(convert, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ") \n",
    "\n",
    "# Aug\n",
    "aug_tilt_train_batches = (\n",
    "    tilt_train_dataset\n",
    "    .take(TRAIN_DATA_LEN)\n",
    "    .cache()\n",
    "    .shuffle(TRAIN_DATA_LEN)\n",
    "    .map(augment, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ") \n",
    "\n",
    "aug_pan_train_batches = (\n",
    "    pan_train_dataset\n",
    "    .take(TRAIN_DATA_LEN)\n",
    "    .cache()\n",
    "    .shuffle(TRAIN_DATA_LEN)\n",
    "    .map(augment, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ") \n",
    "\n",
    "# For final model\n",
    "full_tilt_train_batches = (\n",
    "    tilt_train_dataset\n",
    "    .take(TRAIN_DATA_LEN+VAL_DATA_LEN)\n",
    "    .cache()\n",
    "    .shuffle(TRAIN_DATA_LEN+VAL_DATA_LEN)\n",
    "    .map(augment, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ") \n",
    "\n",
    "full_pan_train_batches = (\n",
    "    pan_train_dataset\n",
    "    .take(TRAIN_DATA_LEN+VAL_DATA_LEN)\n",
    "    .cache()\n",
    "    .shuffle(TRAIN_DATA_LEN+VAL_DATA_LEN)\n",
    "    .map(augment, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ") \n",
    "\n",
    "# Validation\n",
    "tilt_val_batches = (\n",
    "    tilt_val_dataset\n",
    "    .take(VAL_DATA_LEN)\n",
    "    .cache()\n",
    "    .map(convert, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ")\n",
    "\n",
    "pan_val_batches = (\n",
    "    pan_val_dataset\n",
    "    .take(VAL_DATA_LEN)\n",
    "    .cache()\n",
    "    .map(convert, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ") \n",
    "\n",
    "# Submission set\n",
    "test_batches = (\n",
    "    test_dataset\n",
    "    .take(TEST_DATA_LEN)\n",
    "    .cache()\n",
    "    .map(convert, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LT0CnRdpPBMw"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "i=1\n",
    "for image, label in tilt_train_batches.shuffle(100).take(10):\n",
    "  plt.subplot(2,5,i)\n",
    "  plt.imshow(image[0,:])\n",
    "  plt.title(\"Tilt: \" + str(label[0].numpy()[0]))\n",
    "  i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "vEvLybTUPBM8"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "STEPS_PER_EPOCH = TRAIN_DATA_LEN//BATCH_SIZE\n",
    "lr_a = 0.001\n",
    "# lr = 0.0001\n",
    "\n",
    "lr_schedule_a = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    lr_a,\n",
    "    decay_steps=STEPS_PER_EPOCH*1000,\n",
    "    decay_rate=1,\n",
    "    staircase=False\n",
    ")\n",
    "\n",
    "optimizer = Adam(learning_rate=lr_schedule_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Kor2YD5ABnu"
   },
   "source": [
    "An alternate optimisation method using Nesterov's momentum in stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gu4OtK5c-w-_"
   },
   "outputs": [],
   "source": [
    "lr_s = 0.1\n",
    "\n",
    "lr_schedule_s = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "sgd = SGD(learning_rate=0.1, nesterov=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftgxByT0Wp2q"
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLLxLAwEtGdC"
   },
   "source": [
    "A small test to check the a small resnet model works. Nesterov momentum SGD and Adam optimisers are also compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fdkgi3YPBM-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "s_histories_tilt = {}\n",
    "s_histories_pan = {}\n",
    "\n",
    "small_res_net_tilt = get_resnet_model(\"tilt\", [64,], [1,])\n",
    "small_res_net_tilt.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "    'accuracy']\n",
    ")\n",
    "\n",
    "small_res_net_pan = get_resnet_model(\"pan\", [64,], [1,])\n",
    "small_res_net_pan.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "    'accuracy']\n",
    ")\n",
    "\n",
    "small_res_net_tilt_sgd = get_resnet_model(\"tilt\", [64,], [1,])\n",
    "small_res_net_tilt_sgd.compile(\n",
    "    optimizer=sgd, \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "    'accuracy']\n",
    ")\n",
    "\n",
    "small_res_net_pan_sgd = get_resnet_model(\"pan\", [64,], [1,])\n",
    "small_res_net_pan_sgd.compile(\n",
    "    optimizer=sgd, \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "    'accuracy']\n",
    ")\n",
    "\n",
    "s_histories_tilt['tilt_small_resnet_adam'] = small_res_net_tilt.fit(tilt_train_batches,\n",
    "                                              epochs=epochs, \n",
    "                                              validation_data=tilt_val_batches,\n",
    "                                              verbose=0, \n",
    "                                              callbacks=get_callbacks('models/tilt_small_resnet') \n",
    "                                              )\n",
    "s_histories_pan['pan_small_resnet_adam'] = small_res_net_pan.fit(pan_train_batches,\n",
    "                                              epochs=epochs, \n",
    "                                              validation_data=pan_val_batches,\n",
    "                                              verbose=0, \n",
    "                                              callbacks=get_callbacks('models/pan_small_resnet') \n",
    "                                              )\n",
    "\n",
    "s_histories_tilt['tilt_small_resnet_sgd'] = small_res_net_tilt_sgd.fit(tilt_train_batches,\n",
    "                                              epochs=epochs, \n",
    "                                              validation_data=tilt_val_batches,\n",
    "                                              verbose=0, \n",
    "                                              callbacks=get_callbacks('models/tilt_small_resnet') \n",
    "                                              )\n",
    "s_histories_pan['pan_small_resnet_sgd'] = small_res_net_pan_sgd.fit(pan_train_batches,\n",
    "                                              epochs=epochs, \n",
    "                                              validation_data=pan_val_batches,\n",
    "                                              verbose=0, \n",
    "                                              callbacks=get_callbacks('models/pan_small_resnet') \n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_g9XfbdPBNA"
   },
   "outputs": [],
   "source": [
    "plotter(s_histories_tilt, ylim=[0.0, 2], metric = 'CategoricalCrossentropy')\n",
    "plotter(s_histories_tilt, ylim=[0.0, 2], metric = 'CategoricalCrossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3q_WGiSaTHYq"
   },
   "outputs": [],
   "source": [
    "plotter(s_histories_tilt, ylim=[0.0, 1], metric = 'accuracy')\n",
    "plotter(s_histories_pan, ylim=[0.0, 1], metric = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WitvHsuatRV5"
   },
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EwzI4f6pSV4G"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "small_res_net_tilt_aug = get_resnet_model(\"tilt\", [64,], [1,])\n",
    "small_res_net_tilt_aug.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "    'accuracy']\n",
    ")\n",
    "\n",
    "small_res_net_pan_aug = get_resnet_model(\"pan\", [64,], [1,])\n",
    "small_res_net_pan_aug.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "    'accuracy']\n",
    ")\n",
    "\n",
    "s_histories_tilt['tilt_small_resnet_aug'] = small_res_net_tilt_aug.fit(aug_tilt_train_batches,\n",
    "                                              epochs=epochs, \n",
    "                                              validation_data=tilt_val_batches,\n",
    "                                              verbose=0, \n",
    "                                              callbacks=get_callbacks('models/tilt_small_resnet_aug', early_stop=False) \n",
    "                                              )\n",
    "s_histories_pan['pan_small_resnet_aug'] = small_res_net_pan_aug.fit(aug_pan_train_batches,\n",
    "                                              epochs=epochs, \n",
    "                                              validation_data=pan_val_batches,\n",
    "                                              verbose=0, \n",
    "                                              callbacks=get_callbacks('models/pan_small_resnet_aug', early_stop=False) \n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54zQz2txCsy1"
   },
   "outputs": [],
   "source": [
    "plotter(s_histories_tilt, ylim=[0.0, 2], metric = 'CategoricalCrossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zck-jEMuC1Gi"
   },
   "outputs": [],
   "source": [
    "plotter(s_histories_pan, ylim=[0.0, 1], metric = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vh2rvOjstnPF"
   },
   "source": [
    "The learning rate is pretty unsatisfactory. The projected convergence is quite low in loss and accuracy. We can increase the complexity of the model to remedy this.\n",
    "\n",
    "## A baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00_vlWu4TdMj"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "l_histories_tilt = {\n",
    "    'tilt_small_resnet_aug': s_histories_tilt['tilt_small_resnet_aug'],\n",
    "    }\n",
    "\n",
    "l_histories_pan = {\n",
    "    'pan_small_resnet_aug': s_histories_pan['pan_small_resnet_aug'],\n",
    "}\n",
    "\n",
    "large_res_net_tilt = get_resnet_model(\"tilt\", [64, 128, 256], [3, 3, 3])\n",
    "large_res_net_tilt.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "    'accuracy']\n",
    ")\n",
    "\n",
    "large_res_net_pan = get_resnet_model(\"pan\", [64, 128, 256], [3, 3, 3])\n",
    "large_res_net_pan.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "    'accuracy']\n",
    ")\n",
    "\n",
    "l_histories_tilt['tilt_large_resnet'] = large_res_net_tilt.fit(aug_tilt_train_batches,\n",
    "                                              epochs=epochs, \n",
    "                                              validation_data=tilt_val_batches,\n",
    "                                              verbose=0, \n",
    "                                              callbacks=get_callbacks('models/tilt_large_resnet') \n",
    "                                              )\n",
    "l_histories_pan['pan_large_resnet'] = large_res_net_pan.fit(aug_pan_train_batches,\n",
    "                                              epochs=epochs, \n",
    "                                              validation_data=pan_val_batches,\n",
    "                                              verbose=0, \n",
    "                                              callbacks=get_callbacks('models/pan_large_resnet') \n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMfHj_m_UH4S"
   },
   "outputs": [],
   "source": [
    "plotter(l_histories_tilt, ylim=[0.0, 2], metric = 'CategoricalCrossentropy')\n",
    "plotter(l_histories_pan, ylim=[0.0, 2], metric = 'CategoricalCrossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37ypn4_dULCM"
   },
   "outputs": [],
   "source": [
    "plotter(l_histories_tilt, ylim=[0.0, 1], metric = 'accuracy')\n",
    "plotter(l_histories_pan, ylim=[0.0, 1], metric = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jK8CCTdBuHTN"
   },
   "source": [
    "A larger model helps a lot but the validation accuracy looks like it can be improved. Might be some overfitting in the pan model.\n",
    "\n",
    "## Adding depth to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUpWqfEkumxZ"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "larger_histories_tilt = {\n",
    "    'tilt_large_resnet': l_histories_tilt['tilt_large_resnet']\n",
    "}\n",
    "\n",
    "larger_histories_pan = {\n",
    "    'pan_large_resnet': l_histories_pan['pan_large_resnet']\n",
    "}\n",
    "\n",
    "larger_bottleneck_resnet_tilt = get_resnet_model(\"tilt\", [64, 128, 256, 512], [3, 4, 6, 3])\n",
    "larger_bottleneck_resnet_tilt.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "    'accuracy']\n",
    ")\n",
    "\n",
    "larger_bottleneck_resnet_pan = get_resnet_model(\"pan\", [64, 128, 256, 512], [3, 4, 6, 3])\n",
    "larger_bottleneck_resnet_pan.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "    'accuracy']\n",
    ")\n",
    "\n",
    "larger_histories_tilt['larger_resnet_tilt'] = larger_bottleneck_resnet_tilt.fit(aug_tilt_train_batches, \n",
    "                                                          epochs=epochs, \n",
    "                                                          validation_data=tilt_val_batches,\n",
    "                                                          verbose=0, \n",
    "                                                          callbacks=get_callbacks('models/larger_resnet_bottleneck_tilt') )\n",
    "\n",
    "larger_histories_pan['larger_resnet_pan'] = larger_bottleneck_resnet_pan.fit(aug_pan_train_batches, \n",
    "                                                          epochs=epochs, \n",
    "                                                          validation_data=pan_val_batches,\n",
    "                                                          verbose=0, \n",
    "                                                          callbacks=get_callbacks('models/larger_resnet_bottleneck_pan') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad-pHqCIxtvn"
   },
   "outputs": [],
   "source": [
    "plotter(larger_histories_tilt, ylim=[0.0, 3], metric = 'CategoricalCrossentropy')\n",
    "plotter(larger_histories_pan, ylim=[0.0, 3], metric = 'CategoricalCrossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zOI-SdOxtfD"
   },
   "outputs": [],
   "source": [
    "plotter(larger_histories_tilt, ylim=[0.0, 1], metric = 'accuracy')\n",
    "plt.show()\n",
    "plotter(larger_histories_pan, ylim=[0.0, 1], metric = 'accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yiMDzGD0vib"
   },
   "source": [
    "The learning rate looks better but the validation rates look unstable. A model using a modified residual block can help with the stability.\n",
    "\n",
    "## Bottleneck and Preactivation Residual Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2okM4cL1UOy_"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "mod_histories_tilt = {\n",
    "    'tilt_resnet34': larger_histories_tilt['larger_resnet_tilt']\n",
    "    }\n",
    "mod_histories_pan = {\n",
    "    'tilt_resnet34': larger_histories_pan['larger_resnet_pan']\n",
    "}\n",
    "\n",
    "bottleneck_resnet_tilt = get_bottleneck_resnet_model(\"tilt\", [64, 128, 256], [3, 3, 3])\n",
    "bottleneck_resnet_tilt.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "    'accuracy']\n",
    ")\n",
    "\n",
    "bottleneck_resnet_pan = get_bottleneck_resnet_model(\"pan\", [64, 128, 256], [3, 3, 3])\n",
    "bottleneck_resnet_pan.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "    'accuracy']\n",
    ")\n",
    "\n",
    "mod_histories_tilt['tilt_bottleneck_resnet20'] = bottleneck_resnet_tilt.fit(aug_tilt_train_batches,\n",
    "                                              epochs=epochs, \n",
    "                                              validation_data=tilt_val_batches,\n",
    "                                              verbose=0, \n",
    "                                              callbacks=get_callbacks('models/tilt_large_resnet') \n",
    "                                              )\n",
    "mod_histories_pan['pan_bottleneck_resnet20'] = bottleneck_resnet_pan.fit(aug_pan_train_batches,\n",
    "                                              epochs=epochs, \n",
    "                                              validation_data=pan_val_batches,\n",
    "                                              verbose=0, \n",
    "                                              callbacks=get_callbacks('models/pan_large_resnet') \n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uZaycKlDxcQ"
   },
   "outputs": [],
   "source": [
    "plotter(mod_histories_tilt, ylim=[0.0, 2], metric = 'CategoricalCrossentropy')\n",
    "plt.show()\n",
    "plotter(mod_histories_pan, ylim=[0.0, 2], metric = 'CategoricalCrossentropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6rCdcclDzPJ"
   },
   "outputs": [],
   "source": [
    "plotter(mod_histories_tilt, ylim=[0.0, 1], metric = 'accuracy')\n",
    "plt.show()\n",
    "plotter(mod_histories_pan, ylim=[0.0, 1], metric = 'accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2H60ZWZd61I_"
   },
   "source": [
    "Using preactivation and the bottleneck structure in the network stabilises the validation loss and accuracy considerably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uptLLVV34eMt"
   },
   "source": [
    "# Dropout\n",
    "\n",
    "Dropout was initially thought to have a negative effect on training efficiency in ResNet but CITATION proposed an alternative implementation of the network which included dropout after each convolution layer in residual blocks. The dropout within residual blocks is explored in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "soVWMen_6-nh"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "drop_histories = {}\n",
    "drop_histories_tilt = {\n",
    "    'tilt_bottleneck_resnet20': mod_histories_tilt['tilt_bottleneck_resnet20'],\n",
    "}\n",
    "\n",
    "drop_histories_pan = {\n",
    "    'pan_bottleneck_resnet20': mod_histories_pan['pan_bottleneck_resnet20']\n",
    "}\n",
    "\n",
    "bottleneck_res_net_tilt_drop = get_bottleneck_resnet_model(\"tilt\", [64, 128, 256], [3, 3, 3], fdropout=True)\n",
    "bottleneck_res_net_tilt_drop.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "    'accuracy']\n",
    "  )\n",
    "\n",
    "bottleneck_res_net_pan_drop = get_bottleneck_resnet_model(\"pan\", [64, 128, 256], [3, 3, 3], fdropout=True)\n",
    "bottleneck_res_net_pan_drop.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "    'accuracy']\n",
    "  )\n",
    "\n",
    "drop_histories_tilt['tilt_bottleneck_resnet20_dropout'] = bottleneck_res_net_tilt_drop.fit(aug_tilt_train_batches, \n",
    "                                                          epochs=epochs, \n",
    "                                                          validation_data=tilt_val_batches,\n",
    "                                                          verbose=0, \n",
    "                                                          callbacks=get_callbacks('models/resnet_bottleneck_tilt_dropout') )\n",
    "\n",
    "drop_histories_pan['pan_bottleneck_resnet20_dropout'] = bottleneck_res_net_pan_drop.fit(aug_pan_train_batches, \n",
    "                                                          epochs=epochs, \n",
    "                                                          validation_data=pan_val_batches,\n",
    "                                                          verbose=0, \n",
    "                                                          callbacks=get_callbacks('models/resnet_bottleneck_pan_dropout') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVBEX1su9Ma_"
   },
   "outputs": [],
   "source": [
    "plotter(drop_histories_tilt, ylim=[0.0, 2], metric = 'CategoricalCrossentropy')\n",
    "plt.show()\n",
    "plotter(drop_histories_pan, ylim=[0.0, 2], metric = 'CategoricalCrossentropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vmo_0FB9Moa"
   },
   "outputs": [],
   "source": [
    "plotter(drop_histories_tilt, ylim=[0.0, 1], metric = 'accuracy')\n",
    "plt.show()\n",
    "plotter(drop_histories_pan, ylim=[0.0, 1], metric = 'accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2XHhdr8Uf66"
   },
   "source": [
    "Using dropout after convolutional layers provides little benefit in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiHQXTaV9bV1"
   },
   "source": [
    "## Regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tc4p9c4HWfuk"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "h_histories = {}\n",
    "lambda_vals = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "for reg_lambda in lambda_vals:\n",
    "    bottleneck_res_net_tilt_reg_ = get_bottleneck_resnet_model(\"tilt\", [64, 128, 256], [3, 3, 3], reg_lambda=reg_lambda)\n",
    "    bottleneck_res_net_tilt_reg_.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "        'accuracy']\n",
    "    )\n",
    "\n",
    "    bottleneck_res_net_pan_reg_ = get_bottleneck_resnet_model(\"pan\", [64, 128, 256], [3, 3, 3], reg_lambda=reg_lambda)\n",
    "    bottleneck_res_net_pan_reg_.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "        'accuracy']\n",
    "    )\n",
    "\n",
    "    h_histories['resnet_bottleneck_tilt_reg'+ '_h' + str(reg_lambda)] = bottleneck_res_net_tilt_reg_.fit(aug_tilt_train_batches, \n",
    "                                                          epochs=epochs, \n",
    "                                                          validation_data=tilt_val_batches,\n",
    "                                                          verbose=0, \n",
    "                                                          callbacks=get_callbacks('models/resnet_bottleneck_tilt_reg'+ '_h1' + str(reg_lambda)) )\n",
    "\n",
    "\n",
    "    h_histories['resnet_bottleneck_pan_reg'+ '_h' + str(reg_lambda)] = bottleneck_res_net_pan_reg_.fit(aug_pan_train_batches, \n",
    "                                                          epochs=epochs, \n",
    "                                                          validation_data=pan_val_batches,\n",
    "                                                          verbose=0, \n",
    "                                                          callbacks=get_callbacks('models/resnet_bottleneck_pan_reg'+ '_h1' + str(reg_lambda)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ND1qqJrZXTkR"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "metric = 'CategoricalCrossentropy'\n",
    "l_train = list()\n",
    "l_val = list()\n",
    "\n",
    "for reg_lambda in lambda_vals:\n",
    "    l_train.append(h_histories['resnet_bottleneck_tilt_reg'+ '_h' + str(reg_lambda)].history[metric][-1])\n",
    "    l_val.append(h_histories['resnet_bottleneck_tilt_reg'+ '_h' + str(reg_lambda)].history['val_' + metric][-1])\n",
    "\n",
    "plt.plot(lambda_vals,l_train, 'ro', label='Train' )\n",
    "plt.plot(lambda_vals,l_val, 'bs', label='Test' )\n",
    "\n",
    "plt.xlabel('Lambda', fontsize=14)\n",
    "plt.ylabel('CategoricalCrossentropy', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F83ya63XVCfA"
   },
   "outputs": [],
   "source": [
    "lambda_tilt = lambda_vals[np.argmin(l_val)]\n",
    "print(lambda_tilt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2DiVA7PXe2g"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "metric = 'CategoricalCrossentropy'\n",
    "l_train = list()\n",
    "l_val = list()\n",
    "\n",
    "for reg_lambda in lambda_vals:\n",
    "    l_train.append(h_histories['resnet_bottleneck_pan_reg'+ '_h' + str(reg_lambda)].history[metric][-1])\n",
    "    l_val.append(h_histories['resnet_bottleneck_pan_reg'+ '_h' + str(reg_lambda)].history['val_' + metric][-1])\n",
    "\n",
    "plt.plot(lambda_vals,l_train, 'ro', label='Train' )\n",
    "plt.plot(lambda_vals,l_val, 'bs', label='Test' )\n",
    "\n",
    "plt.xlabel('Lambda', fontsize=14)\n",
    "plt.ylabel('CategoricalCrossentropy', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpzVflkHoHop"
   },
   "outputs": [],
   "source": [
    "lambda_pan = lambda_vals[np.argmin(l_val)]\n",
    "print(lambda_pan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NYeyXGu0eM2"
   },
   "source": [
    "Increasing the regularisation strength in the layers results in higher losses and thus we use a minimal amount of regularisation in the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4WJOvq1onUL"
   },
   "outputs": [],
   "source": [
    "final_histories_tilt = {}\n",
    "final_histories_pan = {}\n",
    "\n",
    "final_model_tilt = get_bottleneck_resnet_model(\"tilt\", [64, 128, 256], [3, 3, 3], reg_lambda=lambda_tilt)\n",
    "final_model_tilt.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "    'accuracy']\n",
    ")\n",
    "\n",
    "final_model_pan = get_bottleneck_resnet_model(\"pan\", [64, 128, 256], [3, 3, 3], reg_lambda=lambda_pan)\n",
    "final_model_pan.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "    'accuracy']\n",
    ")\n",
    "\n",
    "final_histories_tilt['tilt_final'] = final_model_tilt.fit(aug_tilt_train_batches,\n",
    "                     epochs=epochs, \n",
    "                     validation_data=tilt_val_batches,\n",
    "                     verbose=0, \n",
    "                     callbacks=get_callbacks('models/tilt_final') \n",
    "                     )\n",
    "final_histories_pan['pan_final'] = final_model_pan.fit(aug_pan_train_batches,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=pan_val_batches,\n",
    "                    verbose=0, \n",
    "                    callbacks=get_callbacks('models/pan_final') \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CKq34Vwho-HI"
   },
   "outputs": [],
   "source": [
    "plotter(final_histories_tilt, ylim=[0.0, 2], metric = 'CategoricalCrossentropy')\n",
    "plt.show()\n",
    "plotter(final_histories_pan, ylim=[0.0, 2], metric = 'CategoricalCrossentropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gveBs4io96_"
   },
   "outputs": [],
   "source": [
    "plotter(final_histories_tilt, ylim=[0.0, 1], metric = 'accuracy')\n",
    "plt.show()\n",
    "plotter(final_histories_pan, ylim=[0.0, 1], metric = 'accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcEDfSwUsPJ7"
   },
   "outputs": [],
   "source": [
    "print(final_model_tilt.evaluate(tilt_val_batches))\n",
    "print(final_model_pan.evaluate(pan_val_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCY30jbR9kpt"
   },
   "source": [
    "## Finalising "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HSBGO2BZdGV"
   },
   "outputs": [],
   "source": [
    "final_model_tilt = get_bottleneck_resnet_model(\"tilt\", [64, 128, 256], [3, 3, 3], reg_lambda=lambda_tilt)\n",
    "final_model_tilt.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "    'accuracy']\n",
    ")\n",
    "\n",
    "final_model_pan = get_bottleneck_resnet_model(\"pan\", [64, 128, 256], [3, 3, 3], reg_lambda=lambda_pan)\n",
    "final_model_pan.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.losses.CategoricalCrossentropy(from_logits=False, name='CategoricalCrossentropy'),\n",
    "    'accuracy']\n",
    ")\n",
    "\n",
    "final_model_tilt.fit(full_tilt_train_batches,\n",
    "                     epochs=epochs, \n",
    "                     verbose=0\n",
    "                     )\n",
    "final_model_pan.fit(full_pan_train_batches,\n",
    "                    epochs=epochs,\n",
    "                    verbose=0\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15HnNvn94bKF"
   },
   "outputs": [],
   "source": [
    "tilt_classes = dict((value, key) for key, value in make_tilt_train_generator().class_indices.items())\n",
    "pan_classes = dict((value, key) for key, value in make_pan_train_generator().class_indices.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8i1t_iXnaAWS"
   },
   "outputs": [],
   "source": [
    "test = make_test_generator()\n",
    "tilt_preds = final_model_tilt.predict(test)\n",
    "pan_preds = final_model_pan.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOgKG-YQLoJO"
   },
   "outputs": [],
   "source": [
    "tilts = [tilt_classes[np.argmax(i)] for i in tilt_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngWLmRtQLrTh"
   },
   "outputs": [],
   "source": [
    "pans = [pan_classes[np.argmax(i)] for i in pan_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gic2k1kzzJ7S"
   },
   "outputs": [],
   "source": [
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "    plot_image(tilt_preds[i], test_data['filename'][i], tilt_classes)\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "    plot_value_array(tilt_preds[i], tilt_classes)\n",
    "    _ = plt.xticks(range(len(tilt_classes)), tilt_classes.values(), rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEh_Ca4b0p1S"
   },
   "outputs": [],
   "source": [
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "    plot_image(i, pan_preds[i], test_data['filename'][i], pan_classes)\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "    plot_value_array(i, pan_preds[i], pan_classes)\n",
    "    _ = plt.xticks(range(len(pan_classes)), pan_classes.values(), rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
